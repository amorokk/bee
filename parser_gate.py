import json
import os
import random
import time
import urllib.parse
import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional

from config import config
from logger import logger, api_logger
from utils import RateLimiter, retry_with_backoff

OUTPUT_FILE = "earn_apr_gt_250.json"

# Rate Limiter –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
rate_limiter = RateLimiter(min_interval=config.MIN_REQUEST_INTERVAL)


class CacheEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫—ç—à–µ —Å –≤—Ä–µ–º–µ–Ω–µ–º —Å–æ–∑–¥–∞–Ω–∏—è"""
    def __init__(self, data: List[Dict], timestamp: float):
        self.data = data
        self.timestamp = timestamp
    
    def is_expired(self, ttl: int = 300) -> bool:
        """TTL –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5 –º–∏–Ω—É—Ç)"""
        return time.time() - self.timestamp > ttl


class ProjectCache:
    """
    –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞.
    –≠–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è –∏ —Å–Ω–∏–∂–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ API.
    """
    def __init__(self):
        self._cache: Dict[float, CacheEntry] = {}  # threshold -> CacheEntry
    
    def get(self, threshold: float, ttl: int = 300) -> Optional[List[Dict]]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞—Ä–µ–ª–∏"""
        entry = self._cache.get(threshold)
        if entry and not entry.is_expired(ttl):
            return entry.data
        return None
    
    def set(self, threshold: float, data: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à"""
        self._cache[threshold] = CacheEntry(data, time.time())
    
    def clear(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à"""
        self._cache.clear()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –ø—Ä–æ–µ–∫—Ç–æ–≤
project_cache = ProjectCache()


def _parse_apr_percent(value: Any) -> Optional[float]:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        cleaned = value.replace("%", "").replace("\xa0", " ")
        cleaned = cleaned.replace(" ", "").replace(",", ".")
        try:
            return float(cleaned)
        except ValueError:
            return None
    return None


def _sort_apr_percent(item: Dict[str, Any]) -> Optional[float]:
    parsed = _parse_apr_percent(item.get("sort_apr"))
    return parsed


def _fetch_page(page_number: int) -> Dict[str, Any]:
    """–ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å retry –∏ rate limiting"""
    def do_request():
        # Rate limiting - –∂–¥–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        rate_limiter.wait_if_needed()
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è human-like –ø–æ–≤–µ–¥–µ–Ω–∏—è
        time.sleep(random.uniform(config.MIN_DELAY, config.MAX_DELAY))
        
        params = {
            "available": "false",
            "limit": str(config.LIMIT_PER_PAGE),
            "have_balance": "2",
            "have_award": "0",
            "is_subscribed": "0",
            "sort_business": "1",
            "search_type": "0",
            "page": str(page_number),
        }
        url = f"{config.BASE_URL}?{urllib.parse.urlencode(params)}"
        
        # Gate.com –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
        with urllib.request.urlopen(url, timeout=config.REQUEST_TIMEOUT) as resp:
            return json.loads(resp.read().decode("utf-8"))
    
    # Retry —Å exponential backoff
    return retry_with_backoff(do_request, max_attempts=config.MAX_RETRIES)


def _fetch_page_with_search(search_coin: str) -> Dict[str, Any]:
    """–ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞ –º–æ–Ω–µ—Ç—ã —Å retry –∏ rate limiting"""
    def do_request():
        # Rate limiting - –∂–¥–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        rate_limiter.wait_if_needed()
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è human-like –ø–æ–≤–µ–¥–µ–Ω–∏—è
        time.sleep(random.uniform(config.MIN_DELAY, config.MAX_DELAY))
        
        params = {
            "search_coin": search_coin,
            "available": "false",
            "limit": str(config.LIMIT_PER_PAGE),
            "have_balance": "2",
            "have_award": "0",
            "is_subscribed": "0",
            "sort_business": "1",
            "search_type": "0",
            "page": "1",
        }
        url = f"{config.BASE_URL}?{urllib.parse.urlencode(params)}"
        
        # Gate.com –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
        with urllib.request.urlopen(url, timeout=config.REQUEST_TIMEOUT) as resp:
            return json.loads(resp.read().decode("utf-8"))
    
    # Retry —Å exponential backoff
    return retry_with_backoff(do_request, max_attempts=3)


def _extract_projects(payload: Dict[str, Any]) -> List[Dict[str, Any]]:
    for key in ("data", "list", "result", "rows"):
        if isinstance(payload.get(key), list):
            return payload[key]
    if isinstance(payload.get("data"), dict):
        for key in ("list", "rows", "data"):
            if isinstance(payload["data"].get(key), list):
                return payload["data"][key]
    return []


def extract_sale_statuses(item: Dict[str, Any]) -> Dict[str, List[int]]:
    statuses: Dict[str, List[int]] = {"fixed_list": [], "fixable_list": []}
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º fixed_list (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π)
    fixed_lst = item.get("fixed_list")
    if isinstance(fixed_lst, list):
        for entry in fixed_lst:
            if isinstance(entry, dict) and "sale_status" in entry:
                statuses["fixed_list"].append(entry["sale_status"])
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º fixable_list (–≥–∏–±–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    fixable_lst = item.get("fixable_list")
    if isinstance(fixable_lst, list):
        for entry in fixable_lst:
            if isinstance(entry, dict) and "sale_status" in entry:
                statuses["fixable_list"].append(entry["sale_status"])
    
    return statuses


def fetch_token_info(search_coin: str) -> Optional[Dict[str, Any]]:
    payload = _fetch_page_with_search(search_coin)
    items = _extract_projects(payload)
    if not items:
        return None
    search_lower = search_coin.strip().lower()
    for item in items:
        asset = str(item.get("asset", "")).lower()
        if asset == search_lower:
            return item
    return items[0] if items else None


def _process_page(page_number: int, threshold: float) -> List[Dict[str, Any]]:
    payload = _fetch_page(page_number)
    items = _extract_projects(payload)
    matched: List[Dict[str, Any]] = []
    for item in items:
        apr_value = _sort_apr_percent(item)
        if apr_value is not None and apr_value > threshold:
            matched.append(item)
    return matched


def fetch_projects_with_apr_gt(threshold: float, force_refresh: bool = False) -> List[Dict[str, Any]]:
    """
    –ó–∞–ø—Ä–æ—Å –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å APR –≤—ã—à–µ threshold —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    
    Args:
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π APR (–≤ –¥–æ–ª—è—Ö, –Ω–µ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
        force_refresh: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –∫—ç—à
    
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –µ—Å–ª–∏ –Ω–µ force_refresh
    if not force_refresh:
        cached = project_cache.get(threshold, ttl=300)  # 5 –º–∏–Ω—É—Ç
        if cached:
            cache_age = int(time.time() - project_cache._cache[threshold].timestamp)
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞: {len(cached)} –ø—Ä–æ–µ–∫—Ç–æ–≤ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ {cache_age} —Å–µ–∫ –Ω–∞–∑–∞–¥)")
            return cached
    
    results: List[Dict[str, Any]] = []
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    pages_to_fetch = config.TOTAL_PAGES
    page_numbers = list(range(1, pages_to_fetch + 1))
    logger.info(f"üîç –ó–∞–ø—Ä–æ—Å {pages_to_fetch} —Å—Ç—Ä–∞–Ω–∏—Ü ({config.MAX_WORKERS} –ø–æ—Ç–æ–∫–æ–≤)...")

    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        futures = {
            executor.submit(_process_page, page_number, threshold): page_number
            for page_number in page_numbers
        }
        completed = 0
        for future in as_completed(futures):
            page_number = futures[future]
            try:
                matched = future.result()
                results.extend(matched)
                completed += 1
                logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}/{pages_to_fetch} ({completed}/{len(page_numbers)}) ‚Äî {len(matched)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            except Exception as exc:
                logger.error(f"‚ùå –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}/{pages_to_fetch} ‚Äî –æ—à–∏–±–∫–∞: {exc}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    project_cache.set(threshold, results)
    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(results)} –ø—Ä–æ–µ–∫—Ç–æ–≤ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—ç—à)")
    return results


if __name__ == "__main__":
    items = fetch_projects_with_apr_gt(2.0)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as fh:
        json.dump(items, fh, ensure_ascii=False, indent=2)
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(items)} –∑–∞–ø–∏—Å–µ–π –≤ {OUTPUT_FILE}")
